# Transformeræ‹‰æ›¼å…‰è°±æ··åˆç‰©åˆ†æ - ä½¿ç”¨æŒ‡å—

> å®Œæ•´çš„é¡¹ç›®ä½¿ç”¨è¯´æ˜

## ğŸ“– å¦‚ä½•é˜…è¯»æœ¬æ–‡æ¡£

**æ ¹æ®æ‚¨çš„ç»éªŒæ°´å¹³é€‰æ‹©é˜…è¯»è·¯å¾„ï¼š**

### ğŸš€ å¿«é€Ÿä¸Šæ‰‹ï¼ˆ5åˆ†é’Ÿï¼‰
1. [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
2. [æ ¸å¿ƒå‘½ä»¤é€ŸæŸ¥](#æ ¸å¿ƒå‘½ä»¤é€ŸæŸ¥)

### ğŸ“š å®Œæ•´å­¦ä¹ ï¼ˆé€‚åˆåˆå­¦è€…ï¼‰
1. [é¡¹ç›®ç®€ä»‹](#é¡¹ç›®ç®€ä»‹)
2. [è¯¦ç»†ä½¿ç”¨æ•™ç¨‹](#è¯¦ç»†ä½¿ç”¨æ•™ç¨‹)
3. [è¾“å‡ºç»“æœè§£è¯»](#è¾“å‡ºç»“æœè§£è¯»)
4. [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)

### ğŸ”§ å¼€å‘å®šåˆ¶ï¼ˆé€‚åˆå¼€å‘è€…ï¼‰
1. [é¡¹ç›®æ¶æ„è§£æ](#é¡¹ç›®æ¶æ„è§£æ)
2. [å‚æ•°é…ç½®è¯¦è§£](#å‚æ•°é…ç½®è¯¦è§£)
3. [å¼€å‘è€…æŒ‡å—](#å¼€å‘è€…æŒ‡å—)

---

## ğŸ“‹ ç›®å½•

- [é¡¹ç›®ç®€ä»‹](#é¡¹ç›®ç®€ä»‹)
- [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
- [æ ¸å¿ƒå‘½ä»¤é€ŸæŸ¥](#æ ¸å¿ƒå‘½ä»¤é€ŸæŸ¥)
- [è¯¦ç»†ä½¿ç”¨æ•™ç¨‹](#è¯¦ç»†ä½¿ç”¨æ•™ç¨‹)
- [è¾“å‡ºç»“æœè§£è¯»](#è¾“å‡ºç»“æœè§£è¯»)
- [é¡¹ç›®æ¶æ„è§£æ](#é¡¹ç›®æ¶æ„è§£æ)
- [å‚æ•°é…ç½®è¯¦è§£](#å‚æ•°é…ç½®è¯¦è§£)
- [å¼€å‘è€…æŒ‡å—](#å¼€å‘è€…æŒ‡å—)
- [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)
- [æ€§èƒ½ä¼˜åŒ–å»ºè®®](#æ€§èƒ½ä¼˜åŒ–å»ºè®®)

---

## é¡¹ç›®ç®€ä»‹

### é¡¹ç›®èƒŒæ™¯

æ‹‰æ›¼å…‰è°±æ˜¯ä¸€ç§éç ´åæ€§çš„åˆ†ææŠ€æœ¯ï¼Œå¹¿æ³›åº”ç”¨äºè¯ç‰©æˆåˆ†åˆ†æã€ææ–™é‰´å®šç­‰é¢†åŸŸã€‚ä¼ ç»Ÿçš„å…‰è°±åˆ†ææ–¹æ³•ä¾èµ–åŒ–å­¦è®¡é‡å­¦å’Œä¸“å®¶ç»éªŒï¼Œè€Œæœ¬é¡¹ç›®åˆ©ç”¨æ·±åº¦å­¦ä¹ ä¸­çš„Transformeræ¶æ„ï¼Œå®ç°äº†è‡ªåŠ¨åŒ–çš„æ··åˆç‰©æˆåˆ†è¯†åˆ«å’Œæµ“åº¦é¢„æµ‹ã€‚

### æŠ€æœ¯ç‰¹ç‚¹

- **è‡ªæ³¨æ„åŠ›æœºåˆ¶**ï¼šTransformerçš„æ ¸å¿ƒä¼˜åŠ¿ï¼Œèƒ½å¤Ÿæ•æ‰å…‰è°±ä¸­ä¸åŒæ³¢é•¿ä¹‹é—´çš„é•¿è·ç¦»ä¾èµ–å…³ç³»
- **ä½ç½®ç¼–ç **ï¼šä¿ç•™æ³¢é•¿ä½ç½®ä¿¡æ¯ï¼Œä½¿æ¨¡å‹ç†è§£å…‰è°±çš„é¡ºåºç»“æ„
- **å¤šä»»åŠ¡å­¦ä¹ **ï¼šåŒæ—¶è®­ç»ƒæˆåˆ†è¯†åˆ«å’Œæµ“åº¦é¢„æµ‹ï¼Œå…±äº«åº•å±‚ç‰¹å¾è¡¨ç¤º
- **ç«¯åˆ°ç«¯å­¦ä¹ **ï¼šä»åŸå§‹å…‰è°±ç›´æ¥é¢„æµ‹ç»“æœï¼Œæ— éœ€æ‰‹å·¥ç‰¹å¾å·¥ç¨‹

### é€‚ç”¨åœºæ™¯

- âœ… è¯ç‰©æ··åˆç‰©å¿«é€Ÿç­›æŸ¥
- âœ… è¯å“è´¨é‡æ§åˆ¶
- âœ… å‡è¯æ£€æµ‹
- âœ… æˆåˆ†å®šé‡åˆ†æ
- âœ… å…‰è°±æ•°æ®æ·±åº¦å­¦ä¹ ç ”ç©¶

### å‰ç½®è¦æ±‚

**å¿…é¡»**ï¼š
- å·²å®Œæˆç¯å¢ƒé…ç½®ï¼ˆå‚è§[ç¯å¢ƒé…ç½®.md](ç¯å¢ƒé…ç½®.md)ï¼‰
- Python 3.8-3.11ç¯å¢ƒ
- åŸºæœ¬çš„Pythonç¼–ç¨‹çŸ¥è¯†

**æ¨è**ï¼ˆéå¿…é¡»ï¼‰ï¼š
- äº†è§£æ·±åº¦å­¦ä¹ åŸºç¡€æ¦‚å¿µ
- äº†è§£PyTorchæ¡†æ¶
- äº†è§£æ‹‰æ›¼å…‰è°±åŸç†

---

## å¿«é€Ÿå¼€å§‹

### ä¸‰æ­¥å¯åŠ¨æµç¨‹

```bash
# æ­¥éª¤1ï¼šæ¿€æ´»ç¯å¢ƒ
conda activate use

# æ­¥éª¤2ï¼šè¿›å…¥é¡¹ç›®ç›®å½•ï¼ˆæ›¿æ¢ä¸ºæ‚¨çš„å®é™…è·¯å¾„ï¼‰
cd "D:\æ—¥å¸¸äº‹åŠ¡\æè€å¸ˆäº‹åŠ¡\2025ç¼–ä¹¦ç³»ç»Ÿå·¥ç¨‹ä¸ç®¡ç†å·¥ç¨‹ä¹¦\2025.10\æ‰€æœ‰ç¨‹åº\Transformeræ¨¡å‹"

# æ­¥éª¤3ï¼šè¿è¡Œå¿«é€Ÿæµ‹è¯•
python quick_start.py
```

### å®Œæ•´å·¥ä½œæµ

```bash
# 1. å¿«é€Ÿæµ‹è¯•ï¼ˆæ¨èé¦–æ¬¡è¿è¡Œï¼‰
python quick_start.py
```

---

## æ ¸å¿ƒå‘½ä»¤é€ŸæŸ¥

### åŸºç¡€æ“ä½œ

```bash
# ç¯å¢ƒæ¿€æ´»
conda activate use

# å¿«é€Ÿæµ‹è¯•ï¼ˆåŒ…å«æ•°æ®ç”Ÿæˆã€æ¨¡å‹åˆ›å»ºã€è®­ç»ƒæ¼”ç¤ºï¼‰
python quick_start.py
```

### é…ç½®æŸ¥çœ‹

```python
# åœ¨Pythonä¸­æŸ¥çœ‹é…ç½®
import config

# æŸ¥çœ‹è®¾å¤‡
print(config.DEVICE)

# æŸ¥çœ‹æ¨¡å‹é…ç½®
print(config.MODEL_CONFIG)

# æŸ¥çœ‹æ•°æ®é…ç½®
print(config.DATA_CONFIG)
```

---

## è¯¦ç»†ä½¿ç”¨æ•™ç¨‹

### æ­¥éª¤1ï¼šå¿«é€Ÿæµ‹è¯•ç¨‹åº

**ç›®çš„**ï¼šéªŒè¯ç¯å¢ƒé…ç½®æ­£ç¡®ï¼Œäº†è§£é¡¹ç›®åŸºæœ¬åŠŸèƒ½

#### è¿è¡Œå‘½ä»¤

```bash
python quick_start.py
```

#### ç¨‹åºæµç¨‹

quick_start.pyæ¼”ç¤ºäº†å®Œæ•´çš„å·¥ä½œæµç¨‹ï¼š

1. **æ•°æ®ç”Ÿæˆæ¼”ç¤º**
   - åˆ›å»º10ä¸ªæ··åˆç‰©æ ·æœ¬
   - æ˜¾ç¤ºå…‰è°±å½¢çŠ¶å’Œæˆåˆ†ä¿¡æ¯

2. **æ¨¡å‹åˆ›å»ºæ¼”ç¤º**
   - åˆå§‹åŒ–Transformeræ¨¡å‹
   - æ˜¾ç¤ºæ¨¡å‹å‚æ•°é‡ï¼ˆçº¦81ä¸‡å‚æ•°ï¼‰

3. **å‰å‘ä¼ æ’­æ¼”ç¤º**
   - å¯¹4ä¸ªæ ·æœ¬è¿›è¡Œé¢„æµ‹
   - æ˜¾ç¤ºè¾“å‡ºå½¢çŠ¶å’Œç¤ºä¾‹ç»“æœ

4. **ç®€å•è®­ç»ƒæ¼”ç¤º**
   - ç”Ÿæˆ20ä¸ªè®­ç»ƒæ ·æœ¬
   - è®­ç»ƒ5ä¸ªepoch
   - æ˜¾ç¤ºæŸå¤±å˜åŒ–

#### è¾“å‡ºç¤ºä¾‹

```
æ¬¢è¿ä½¿ç”¨Transformeræ‹‰æ›¼å…‰è°±æ··åˆç‰©åˆ†æç³»ç»Ÿ
ä½¿ç”¨è®¾å¤‡: cpu

================================================================================
1. æ•°æ®ç”Ÿæˆæ¼”ç¤º
================================================================================

ç”Ÿæˆçš„æ•°æ®:
  å…‰è°±å½¢çŠ¶: (10, 1738)
  æˆåˆ†æ ‡ç­¾å½¢çŠ¶: (10, 5)
  æµ“åº¦å½¢çŠ¶: (10, 5)
  å…‰è°±èŒƒå›´: [-0.123, 1.456]

å‰3ä¸ªæ··åˆç‰©çš„æˆåˆ†:

  æ··åˆç‰©1:
    å¯¹ä¹™é…°æ°¨åŸºé…š: 0.523
    å’–å•¡å› : 0.477
    
  æ··åˆç‰©2:
    é˜¿å¸åŒ¹æ—: 1.000
    
  æ··åˆç‰©3:
    å¸ƒæ´›èŠ¬: 0.631
    è˜æ™®ç”Ÿ: 0.369
...
```

#### é‡ç‚¹è¯´æ˜

- **è®¾å¤‡ä¿¡æ¯**ï¼šæ˜¾ç¤ºå½“å‰ä½¿ç”¨CPUè¿˜æ˜¯GPU
- **æ•°æ®å½¢çŠ¶**ï¼š(æ ·æœ¬æ•°, ç‰¹å¾ç»´åº¦)
- **æ¨¡å‹å‚æ•°**ï¼šå‚æ•°è¶Šå¤šï¼Œæ¨¡å‹å®¹é‡è¶Šå¤§ï¼Œä½†è®­ç»ƒæ—¶é—´è¶Šé•¿
- **æŸå¤±å˜åŒ–**ï¼šæ­£å¸¸æƒ…å†µä¸‹æŸå¤±åº”è¯¥é€æ¸ä¸‹é™

---

### æ­¥éª¤2ï¼šæ•°æ®ç”Ÿæˆä¸é¢„å¤„ç†

**ç›®çš„**ï¼šäº†è§£å¦‚ä½•ç”Ÿæˆå’Œå¤„ç†æ‹‰æ›¼å…‰è°±æ•°æ®

#### æ•°æ®ç”Ÿæˆ

```python
from data import RamanMixtureGenerator
import config

# åˆ›å»ºæ•°æ®ç”Ÿæˆå™¨
generator = RamanMixtureGenerator(
    num_mixtures=config.NUM_MIXTURES,  # 80ä¸ªæ··åˆç‰©
    random_seed=42  # å›ºå®šéšæœºç§å­ä»¥ä¿è¯å¯é‡å¤æ€§
)

# ç”Ÿæˆæ•°æ®é›†
spectra, component_labels, concentrations = generator.generate_dataset()

print(f"å…‰è°±å½¢çŠ¶: {spectra.shape}")  # (80, 1738)
print(f"æˆåˆ†æ ‡ç­¾å½¢çŠ¶: {component_labels.shape}")  # (80, 5)
print(f"æµ“åº¦å½¢çŠ¶: {concentrations.shape}")  # (80, 5)
```

#### æ•°æ®æ ¼å¼è¯´æ˜

- **spectra**: æ‹‰æ›¼å…‰è°±æ•°æ®ï¼Œå½¢çŠ¶ä¸º(N, 1738)
  - N: æ ·æœ¬æ•°é‡
  - 1738: æ³¢æ•°èŒƒå›´400-2000 cmâ»Â¹çš„é‡‡æ ·ç‚¹æ•°

- **component_labels**: æˆåˆ†æ ‡ç­¾ï¼Œå½¢çŠ¶ä¸º(N, 5)
  - å¤šæ ‡ç­¾åˆ†ç±»æ ¼å¼
  - 1è¡¨ç¤ºè¯¥æˆåˆ†å­˜åœ¨ï¼Œ0è¡¨ç¤ºä¸å­˜åœ¨
  - ç¤ºä¾‹ï¼š[1, 1, 0, 0, 0] è¡¨ç¤ºåŒ…å«é˜¿å¸åŒ¹æ—å’Œå¯¹ä¹™é…°æ°¨åŸºé…š

- **concentrations**: æµ“åº¦å€¼ï¼Œå½¢çŠ¶ä¸º(N, 5)
  - å½’ä¸€åŒ–åçš„ç›¸å¯¹æµ“åº¦
  - æ‰€æœ‰æˆåˆ†æµ“åº¦ä¹‹å’Œä¸º1.0
  - ç¤ºä¾‹ï¼š[0.6, 0.4, 0, 0, 0] è¡¨ç¤º60%é˜¿å¸åŒ¹æ—å’Œ40%å¯¹ä¹™é…°æ°¨åŸºé…š

#### æ•°æ®é¢„å¤„ç†

```python
from utils.preprocessing import preprocess_spectrum

# é¢„å¤„ç†å…‰è°±ï¼ˆåŸºçº¿æ ¡æ­£ + å½’ä¸€åŒ–ï¼‰
processed_spectra = preprocess_spectrum(
    spectra,
    baseline_correction_method='poly',  # å¤šé¡¹å¼åŸºçº¿æ ¡æ­£
    normalization_method='minmax',      # æœ€å°-æœ€å¤§å½’ä¸€åŒ–
    degree=3                            # å¤šé¡¹å¼é˜¶æ•°
)

# å¯é€‰ï¼šæ ‡å‡†å½’ä¸€åŒ–
processed_spectra_std = preprocess_spectrum(
    spectra,
    baseline_correction_method='als',   # è‡ªé€‚åº”æœ€å°äºŒä¹˜æ³•
    normalization_method='standard'     # æ ‡å‡†å½’ä¸€åŒ–ï¼ˆå‡å€¼0ï¼Œæ–¹å·®1ï¼‰
)
```

#### æ•°æ®å¯è§†åŒ–

```python
import matplotlib.pyplot as plt
import config

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei']
plt.rcParams['axes.unicode_minus'] = False

# åˆ›å»ºæ³¢æ•°åæ ‡
wavenumbers = np.linspace(
    config.RAMAN_SHIFT_RANGE[0],
    config.RAMAN_SHIFT_RANGE[1],
    config.INPUT_DIM
)

# ç»˜åˆ¶åŸå§‹å…‰è°±vsé¢„å¤„ç†åå…‰è°±
fig, axes = plt.subplots(2, 1, figsize=(12, 8))

# åŸå§‹å…‰è°±
axes[0].plot(wavenumbers, spectra[0])
axes[0].set_title('åŸå§‹æ‹‰æ›¼å…‰è°±')
axes[0].set_xlabel('æ³¢æ•° (cmâ»Â¹)')
axes[0].set_ylabel('å¼ºåº¦')
axes[0].grid(True, alpha=0.3)

# é¢„å¤„ç†åå…‰è°±
axes[1].plot(wavenumbers, processed_spectra[0])
axes[1].set_title('é¢„å¤„ç†åå…‰è°±')
axes[1].set_xlabel('æ³¢æ•° (cmâ»Â¹)')
axes[1].set_ylabel('å½’ä¸€åŒ–å¼ºåº¦')
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('spectrum_comparison.png', dpi=150, transparent=True)
plt.close()
```

---

### æ­¥éª¤3ï¼šæ¨¡å‹åˆ›å»ºä¸è®­ç»ƒ

**ç›®çš„**ï¼šäº†è§£å¦‚ä½•åˆ›å»ºå’Œè®­ç»ƒTransformeræ¨¡å‹

#### åˆ›å»ºæ¨¡å‹

```python
from models import create_model
import config

# åˆ›å»ºæ¨¡å‹
model = create_model()

# ç§»åŠ¨åˆ°è®¾å¤‡
model = model.to(config.DEVICE)

# æŸ¥çœ‹æ¨¡å‹ä¿¡æ¯
model_info = model.get_model_info()
print("æ¨¡å‹ä¿¡æ¯:")
for key, value in model_info.items():
    if isinstance(value, int):
        print(f"  {key}: {value:,}")
    else:
        print(f"  {key}: {value}")
```

#### å‡†å¤‡æ•°æ®é›†

```python
from utils.dataset import SpectrumDataset
from torch.utils.data import DataLoader
import torch

# é¢„å¤„ç†æ•°æ®
from utils.preprocessing import preprocess_spectrum
spectra_processed = preprocess_spectrum(spectra, 'poly', 'minmax', degree=3)

# åˆ›å»ºæ•°æ®é›†
dataset = SpectrumDataset(spectra_processed, component_labels, concentrations)

# åˆ’åˆ†è®­ç»ƒé›†ã€éªŒè¯é›†ã€æµ‹è¯•é›†
train_size = int(0.7 * len(dataset))
val_size = int(0.15 * len(dataset))
test_size = len(dataset) - train_size - val_size

train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(
    dataset, [train_size, val_size, test_size]
)

# åˆ›å»ºæ•°æ®åŠ è½½å™¨
train_loader = DataLoader(
    train_dataset,
    batch_size=config.BATCH_SIZE,
    shuffle=True
)

val_loader = DataLoader(
    val_dataset,
    batch_size=config.BATCH_SIZE,
    shuffle=False
)

test_loader = DataLoader(
    test_dataset,
    batch_size=config.BATCH_SIZE,
    shuffle=False
)

print(f"è®­ç»ƒé›†: {len(train_dataset)} æ ·æœ¬")
print(f"éªŒè¯é›†: {len(val_dataset)} æ ·æœ¬")
print(f"æµ‹è¯•é›†: {len(test_dataset)} æ ·æœ¬")
```

#### å®Œæ•´è®­ç»ƒæµç¨‹

```python
from utils.metrics import evaluate_model, MultiTaskMetrics, calculate_loss
import torch.optim as optim

# å®šä¹‰ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨
optimizer = optim.Adam(
    model.parameters(),
    lr=config.LEARNING_RATE,
    weight_decay=config.WEIGHT_DECAY
)

scheduler = optim.lr_scheduler.ReduceLROnPlateau(
    optimizer,
    mode='min',
    factor=config.LR_SCHEDULER['factor'],
    patience=config.LR_SCHEDULER['patience'],
    min_lr=config.LR_SCHEDULER['min_lr']
)

# æ—©åœæœºåˆ¶
best_val_loss = float('inf')
patience_counter = 0

# è®­ç»ƒå¾ªç¯
for epoch in range(1, config.EPOCHS + 1):
    # ===== è®­ç»ƒé˜¶æ®µ =====
    model.train()
    train_loss = 0.0
    
    for batch_spectra, batch_labels, batch_conc in train_loader:
        # ç§»åŠ¨åˆ°è®¾å¤‡
        batch_spectra = batch_spectra.to(config.DEVICE)
        batch_labels = batch_labels.to(config.DEVICE)
        batch_conc = batch_conc.to(config.DEVICE)
        
        # å‰å‘ä¼ æ’­
        optimizer.zero_grad()
        comp_pred, conc_pred = model(batch_spectra)
        
        # è®¡ç®—æŸå¤±
        loss, class_loss, reg_loss = calculate_loss(
            comp_pred, batch_labels,
            conc_pred, batch_conc
        )
        
        # åå‘ä¼ æ’­
        loss.backward()
        optimizer.step()
        
        train_loss += loss.item()
    
    train_loss /= len(train_loader)
    
    # ===== éªŒè¯é˜¶æ®µ =====
    metrics, val_loss, _, _ = evaluate_model(model, val_loader, config.DEVICE)
    
    # å­¦ä¹ ç‡è°ƒåº¦
    scheduler.step(val_loss)
    
    # æ‰“å°è¿›åº¦
    if epoch % 5 == 0:
        print(f"Epoch {epoch}/{config.EPOCHS}")
        print(f"  è®­ç»ƒæŸå¤±: {train_loss:.4f}")
        print(f"  éªŒè¯æŸå¤±: {val_loss:.4f}")
        print(f"  éªŒè¯F1: {metrics['classification']['f1']:.4f}")
        print(f"  éªŒè¯RÂ²: {metrics['regression']['r2']:.4f}")
    
    # æ—©åœæ£€æŸ¥
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        patience_counter = 0
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        torch.save(model.state_dict(), config.MODEL_SAVE_PATH)
        print(f"  â†’ ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆéªŒè¯æŸå¤±: {val_loss:.4f}ï¼‰")
    else:
        patience_counter += 1
        if patience_counter >= config.EARLY_STOPPING_PATIENCE:
            print(f"\næ—©åœè§¦å‘ï¼å·²{patience_counter}ä¸ªepochæ— æ”¹å–„")
            break

print("\nè®­ç»ƒå®Œæˆï¼")
```

---

### æ­¥éª¤4ï¼šæ¨¡å‹è¯„ä¼°

**ç›®çš„**ï¼šè¯„ä¼°æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„æ€§èƒ½

```python
from utils.metrics import evaluate_model, MultiTaskMetrics

# åŠ è½½æœ€ä½³æ¨¡å‹
model.load_state_dict(torch.load(config.MODEL_SAVE_PATH))

# è¯„ä¼°
metrics, test_loss, class_loss, reg_loss = evaluate_model(
    model, test_loader, config.DEVICE
)

# æ‰“å°è¯„ä¼°ç»“æœ
metrics_calc = MultiTaskMetrics()
print("\n" + "=" * 80)
print("æµ‹è¯•é›†è¯„ä¼°ç»“æœ")
print("=" * 80)
metrics_calc.print_metrics(metrics, prefix="æµ‹è¯•é›† ")
```

---

### æ­¥éª¤5ï¼šæ¨¡å‹æ¨ç†

**ç›®çš„**ï¼šä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œé¢„æµ‹

```python
import numpy as np
import torch

# åŠ è½½æ¨¡å‹
model = create_model()
model.load_state_dict(torch.load(config.MODEL_SAVE_PATH))
model.eval()
model = model.to(config.DEVICE)

# å‡†å¤‡å•ä¸ªæ ·æœ¬
sample_spectrum = spectra_processed[0]  # å–ç¬¬ä¸€ä¸ªæ ·æœ¬
sample_tensor = torch.FloatTensor(sample_spectrum).unsqueeze(0).to(config.DEVICE)

# é¢„æµ‹
with torch.no_grad():
    component_pred, concentration_pred = model(sample_tensor)

# è½¬æ¢ä¸ºnumpy
component_pred = component_pred.cpu().numpy()[0]
concentration_pred = concentration_pred.cpu().numpy()[0]

# äºŒå€¼åŒ–æˆåˆ†é¢„æµ‹ï¼ˆé˜ˆå€¼0.5ï¼‰
component_binary = (component_pred > 0.5).astype(int)

# æ‰“å°ç»“æœ
print("\né¢„æµ‹ç»“æœ:")
print("=" * 50)
for i, name in enumerate(config.COMPONENT_NAMES):
    if component_binary[i] == 1:
        print(f"  {name}:")
        print(f"    å­˜åœ¨æ¦‚ç‡: {component_pred[i]:.3f}")
        print(f"    é¢„æµ‹æµ“åº¦: {concentration_pred[i]:.3f}")

# æ‰“å°çœŸå®å€¼ï¼ˆå¦‚æœæœ‰ï¼‰
print("\nçœŸå®å€¼:")
print("=" * 50)
for i, name in enumerate(config.COMPONENT_NAMES):
    if component_labels[0, i] == 1:
        print(f"  {name}:")
        print(f"    çœŸå®æµ“åº¦: {concentrations[0, i]:.3f}")
```

---

## è¾“å‡ºç»“æœè§£è¯»

### åˆ†ç±»æŒ‡æ ‡

#### Exact Match Ratioï¼ˆå®Œå…¨åŒ¹é…ç‡ï¼‰

**å®šä¹‰**ï¼šæ‰€æœ‰æˆåˆ†æ ‡ç­¾éƒ½é¢„æµ‹æ­£ç¡®çš„æ ·æœ¬æ¯”ä¾‹

**è®¡ç®—å…¬å¼**ï¼š
$$\text{EMR} = \frac{1}{N}\sum_{i=1}^{N}\mathbb{1}(\hat{y}_i = y_i)$$

å¼ä¸­ï¼Œ$$N$$ä¸ºæ ·æœ¬æ•°ï¼›$$\hat{y}_i$$ä¸ºç¬¬$$i$$ä¸ªæ ·æœ¬çš„é¢„æµ‹æ ‡ç­¾ï¼›$$y_i$$ä¸ºçœŸå®æ ‡ç­¾ï¼›$$\mathbb{1}(\cdot)$$ä¸ºæŒ‡ç¤ºå‡½æ•°ã€‚

**æ„ä¹‰**ï¼š
- è¡¡é‡å¤šæ ‡ç­¾åˆ†ç±»çš„ä¸¥æ ¼å‡†ç¡®æ€§
- å€¼è¶Šæ¥è¿‘1è¶Šå¥½
- æœŸæœ›å€¼ï¼š>0.80ä¸ºä¼˜ç§€

#### Hamming Lossï¼ˆæ±‰æ˜æŸå¤±ï¼‰

**å®šä¹‰**ï¼šå¹³å‡æ¯ä¸ªæ ·æœ¬é¢„æµ‹é”™è¯¯çš„æ ‡ç­¾æ¯”ä¾‹

**è®¡ç®—å…¬å¼**ï¼š
$$\text{Hamming Loss} = \frac{1}{N \times L}\sum_{i=1}^{N}\sum_{j=1}^{L}\mathbb{1}(\hat{y}_{ij} \neq y_{ij})$$

å¼ä¸­ï¼Œ$$L$$ä¸ºæ ‡ç­¾æ•°ï¼ˆæœ¬é¡¹ç›®ä¸­$$L=5$$ï¼‰ï¼›$$\hat{y}_{ij}$$ä¸ºç¬¬$$i$$ä¸ªæ ·æœ¬ç¬¬$$j$$ä¸ªæ ‡ç­¾çš„é¢„æµ‹å€¼ã€‚

**æ„ä¹‰**ï¼š
- è¡¡é‡æ ‡ç­¾çº§åˆ«çš„å¹³å‡é”™è¯¯ç‡
- å€¼è¶Šæ¥è¿‘0è¶Šå¥½
- æœŸæœ›å€¼ï¼š<0.15ä¸ºä¼˜ç§€

#### F1-scoreï¼ˆF1åˆ†æ•°ï¼‰

**å®šä¹‰**ï¼šç²¾ç¡®ç‡å’Œå¬å›ç‡çš„è°ƒå’Œå¹³å‡

**è®¡ç®—å…¬å¼**ï¼š
$$\text{F1} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}$$

**æ„ä¹‰**ï¼š
- ç»¼åˆè€ƒè™‘ç²¾ç¡®ç‡å’Œå¬å›ç‡
- å€¼è¶Šæ¥è¿‘1è¶Šå¥½
- æœŸæœ›å€¼ï¼š>0.85ä¸ºä¼˜ç§€

### å›å½’æŒ‡æ ‡

#### RMSEï¼ˆå‡æ–¹æ ¹è¯¯å·®ï¼‰

**å®šä¹‰**ï¼šé¢„æµ‹å€¼ä¸çœŸå®å€¼å·®çš„å¹³æ–¹çš„å¹³å‡å€¼çš„å¹³æ–¹æ ¹

**è®¡ç®—å…¬å¼**ï¼š
$$\text{RMSE} = \sqrt{\frac{1}{N \times L}\sum_{i=1}^{N}\sum_{j=1}^{L}(\hat{c}_{ij} - c_{ij})^2}$$

å¼ä¸­ï¼Œ$$\hat{c}_{ij}$$ä¸ºç¬¬$$i$$ä¸ªæ ·æœ¬ç¬¬$$j$$ä¸ªæˆåˆ†çš„é¢„æµ‹æµ“åº¦ï¼›$$c_{ij}$$ä¸ºçœŸå®æµ“åº¦ã€‚

**æ„ä¹‰**ï¼š
- è¡¡é‡æµ“åº¦é¢„æµ‹çš„å¹³å‡è¯¯å·®
- å•ä½ä¸æµ“åº¦ç›¸åŒ
- æœŸæœ›å€¼ï¼š<0.10ä¸ºä¼˜ç§€ï¼ˆç›¸å¯¹æµ“åº¦ï¼‰

#### MAEï¼ˆå¹³å‡ç»å¯¹è¯¯å·®ï¼‰

**å®šä¹‰**ï¼šé¢„æµ‹å€¼ä¸çœŸå®å€¼å·®çš„ç»å¯¹å€¼çš„å¹³å‡

**è®¡ç®—å…¬å¼**ï¼š
$$\text{MAE} = \frac{1}{N \times L}\sum_{i=1}^{N}\sum_{j=1}^{L}|\hat{c}_{ij} - c_{ij}|$$

**æ„ä¹‰**ï¼š
- è¡¡é‡æµ“åº¦é¢„æµ‹çš„å¹³å‡ç»å¯¹åå·®
- å¯¹å¼‚å¸¸å€¼ä¸å¦‚RMSEæ•æ„Ÿ
- æœŸæœ›å€¼ï¼š<0.08ä¸ºä¼˜ç§€ï¼ˆç›¸å¯¹æµ“åº¦ï¼‰

#### RÂ²ï¼ˆå†³å®šç³»æ•°ï¼‰

**å®šä¹‰**ï¼šæ¨¡å‹è§£é‡Šçš„æ–¹å·®å æ€»æ–¹å·®çš„æ¯”ä¾‹

**è®¡ç®—å…¬å¼**ï¼š
$$R^2 = 1 - \frac{\sum_{i,j}(\hat{c}_{ij} - c_{ij})^2}{\sum_{i,j}(c_{ij} - \bar{c})^2}$$

å¼ä¸­ï¼Œ$$\bar{c}$$ä¸ºæ‰€æœ‰æµ“åº¦å€¼çš„å¹³å‡å€¼ã€‚

**æ„ä¹‰**ï¼š
- è¡¡é‡æ¨¡å‹çš„æ‹Ÿåˆä¼˜åº¦
- å€¼è¶Šæ¥è¿‘1è¶Šå¥½
- æœŸæœ›å€¼ï¼š>0.90ä¸ºä¼˜ç§€

---

## é¡¹ç›®æ¶æ„è§£æ

### ç›®å½•ç»“æ„è¯¦è§£

```
Transformeræ¨¡å‹/
â”‚
â”œâ”€â”€ config.py                 # å…¨å±€é…ç½®æ–‡ä»¶
â”‚   â””â”€â”€ åŒ…å«æ‰€æœ‰è¶…å‚æ•°ã€è·¯å¾„ã€è®¾å¤‡é…ç½®
â”‚
â”œâ”€â”€ quick_start.py            # å¿«é€Ÿå¼€å§‹è„šæœ¬
â”‚   â””â”€â”€ æ¼”ç¤ºå®Œæ•´å·¥ä½œæµç¨‹
â”‚
â”œâ”€â”€ models/                   # æ¨¡å‹å®šä¹‰æ¨¡å—
â”‚   â”œâ”€â”€ __init__.py           # æ¨¡å—å¯¼å‡º
â”‚   â”œâ”€â”€ transformer.py        # Transformeræ¨¡å‹ç±»
â”‚   â”‚   â””â”€â”€ SpectrumTransformer: ä¸»æ¨¡å‹ç±»
â”‚   â””â”€â”€ positional_encoding.py # ä½ç½®ç¼–ç æ¨¡å—
â”‚       â””â”€â”€ PositionalEncoding: æ­£å¼¦ä½™å¼¦ä½ç½®ç¼–ç 
â”‚
â”œâ”€â”€ data/                     # æ•°æ®å¤„ç†æ¨¡å—
â”‚   â”œâ”€â”€ __init__.py           # æ¨¡å—å¯¼å‡º
â”‚   â””â”€â”€ data_generator.py     # æ•°æ®ç”Ÿæˆå™¨
â”‚       â””â”€â”€ RamanMixtureGenerator: æ¨¡æ‹Ÿæ‹‰æ›¼å…‰è°±æ•°æ®
â”‚
â”œâ”€â”€ utils/                    # å·¥å…·å‡½æ•°æ¨¡å—
â”‚   â”œâ”€â”€ __init__.py           # æ¨¡å—å¯¼å‡º
â”‚   â”œâ”€â”€ dataset.py            # PyTorchæ•°æ®é›†ç±»
â”‚   â”‚   â””â”€â”€ SpectrumDataset: å…‰è°±æ•°æ®é›†å°è£…
â”‚   â”œâ”€â”€ metrics.py            # è¯„ä¼°æŒ‡æ ‡è®¡ç®—
â”‚   â”‚   â”œâ”€â”€ calculate_loss: æŸå¤±å‡½æ•°
â”‚   â”‚   â”œâ”€â”€ evaluate_model: æ¨¡å‹è¯„ä¼°
â”‚   â”‚   â””â”€â”€ MultiTaskMetrics: æŒ‡æ ‡è®¡ç®—ç±»
â”‚   â””â”€â”€ preprocessing.py      # æ•°æ®é¢„å¤„ç†
â”‚       â””â”€â”€ preprocess_spectrum: åŸºçº¿æ ¡æ­£+å½’ä¸€åŒ–
â”‚
â”œâ”€â”€ saved_models/             # æ¨¡å‹ä¿å­˜ç›®å½•
â”‚   â””â”€â”€ transformer_best.pth  # æœ€ä½³æ¨¡å‹æƒé‡
â”‚
â”œâ”€â”€ results/                  # ç»“æœè¾“å‡ºç›®å½•
â”œâ”€â”€ logs/                     # æ—¥å¿—æ–‡ä»¶ç›®å½•
â””â”€â”€ docs/                     # æ–‡æ¡£ç›®å½•
```

### æ•°æ®æµå‘

```mermaid
graph TD
    A[åŸå§‹å…‰è°±æ•°æ®] --> B[æ•°æ®é¢„å¤„ç†]
    B --> C[åŸºçº¿æ ¡æ­£]
    C --> D[å½’ä¸€åŒ–]
    D --> E[æ•°æ®é›†åˆ’åˆ†]
    E --> F[DataLoader]
    F --> G[Transformeræ¨¡å‹]
    G --> H[åµŒå…¥å±‚]
    H --> I[ä½ç½®ç¼–ç ]
    I --> J[Transformer Encoder]
    J --> K[å…¨å±€æ± åŒ–]
    K --> L[è¾“å‡ºå±‚]
    L --> M[æˆåˆ†é¢„æµ‹]
    L --> N[æµ“åº¦é¢„æµ‹]
    M --> O[æŸå¤±è®¡ç®—]
    N --> O
    O --> P[åå‘ä¼ æ’­]
    P --> Q[å‚æ•°æ›´æ–°]
```

### æ¨¡å‹æ¶æ„è¯¦è§£

#### Transformer Encoderæ¶æ„

```
è¾“å…¥: (batch_size, seq_len=1738, d_model=1)
    â†“
[1] åµŒå…¥å±‚ (Linear)
    è¾“å…¥: (batch, 1738, 1)
    è¾“å‡º: (batch, 1738, 128)
    å‚æ•°: 1Ã—128 = 128
    â†“
[2] ä½ç½®ç¼–ç  (PositionalEncoding)
    ä½¿ç”¨æ­£å¼¦ä½™å¼¦å‡½æ•°ç¼–ç ä½ç½®ä¿¡æ¯
    è¾“å‡º: (batch, 1738, 128)
    å‚æ•°: 0ï¼ˆæ— å¯å­¦ä¹ å‚æ•°ï¼‰
    â†“
[3] Transformer Encoder Layer Ã— 4
    â”œâ”€ å¤šå¤´è‡ªæ³¨æ„åŠ› (MultiheadAttention)
    â”‚   head_dim = d_model / nhead = 128 / 8 = 16
    â”‚   Q, K, VçŸ©é˜µ: 3 Ã— (128Ã—128) = 49,152
    â”‚   è¾“å‡ºæŠ•å½±: 128Ã—128 = 16,384
    â”‚   æ¯å±‚å‚æ•°: 65,536
    â”‚
    â”œâ”€ Add & LayerNorm
    â”‚   å‚æ•°: 2Ã—128 = 256
    â”‚
    â”œâ”€ å‰é¦ˆç½‘ç»œ (Feed-Forward)
    â”‚   Linear1: 128Ã—512 = 65,536
    â”‚   Linear2: 512Ã—128 = 65,536
    â”‚   æ¯å±‚å‚æ•°: 131,072
    â”‚
    â””â”€ Add & LayerNorm
        å‚æ•°: 2Ã—128 = 256
    
    å•å±‚æ€»å‚æ•°: 197,120
    4å±‚æ€»å‚æ•°: 788,480
    â†“
[4] å…¨å±€å¹³å‡æ± åŒ– (Global Average Pooling)
    è¾“å…¥: (batch, 1738, 128)
    è¾“å‡º: (batch, 128)
    å‚æ•°: 0
    â†“
[5] åˆ†ç±»å¤´ (Classification Head)
    Linear1: 128Ã—64 = 8,192
    ReLU
    Dropout(0.3)
    Linear2: 64Ã—5 = 320
    Sigmoid
    æ€»å‚æ•°: 8,512
    â†“
[6] å›å½’å¤´ (Regression Head)
    Linear1: 128Ã—64 = 8,192
    ReLU
    Dropout(0.3)
    Linear2: 64Ã—5 = 320
    Softmaxï¼ˆå½’ä¸€åŒ–ä¿è¯æµ“åº¦å’Œä¸º1ï¼‰
    æ€»å‚æ•°: 8,512

æ€»å‚æ•°é‡: 128 + 788,480 + 8,512 + 8,512 = 805,632
å®é™…: 810,506ï¼ˆåŒ…å«LayerNormçš„biasï¼‰
```

**ç‰¹ç‚¹**ï¼š
- è‡ªæ³¨æ„åŠ›æœºåˆ¶æ•æ‰å…¨å±€ä¾èµ–
- å¤šå¤´æ³¨æ„åŠ›æä¾›å¤šè§’åº¦ç‰¹å¾æå–
- æ®‹å·®è¿æ¥ç¼“è§£æ¢¯åº¦æ¶ˆå¤±
- LayerNormåŠ é€Ÿè®­ç»ƒæ”¶æ•›

---

## å‚æ•°é…ç½®è¯¦è§£

æ‰€æœ‰é…ç½®å‚æ•°åœ¨`config.py`æ–‡ä»¶ä¸­é›†ä¸­ç®¡ç†ã€‚

### æ•°æ®é…ç½®

```python
# æ‹‰æ›¼å…‰è°±é…ç½®
INPUT_DIM = 1738  # é‡‡æ ·ç‚¹æ•°é‡
RAMAN_SHIFT_RANGE = (400, 2000)  # æ³¢æ•°èŒƒå›´
NUM_MIXTURES = 80  # æ··åˆç‰©æ•°é‡

# æˆåˆ†é…ç½®
COMPONENT_NAMES = [
    'é˜¿å¸åŒ¹æ—',
    'å¯¹ä¹™é…°æ°¨åŸºé…š',
    'å’–å•¡å› ',
    'å¸ƒæ´›èŠ¬',
    'è˜æ™®ç”Ÿ',
]
NUM_COMPONENTS = 5
```

**ä¿®æ”¹å»ºè®®**ï¼š

| å‚æ•° | å¢å¤§æ•ˆæœ | å‡å°æ•ˆæœ | æ¨èå€¼ |
|------|----------|----------|--------|
| NUM_MIXTURES | æ•°æ®é‡å¢åŠ ï¼Œè®­ç»ƒæ—¶é—´é•¿ | è®­ç»ƒå¿«ä½†å¯èƒ½è¿‡æ‹Ÿåˆ | 80-200 |
| NUM_COMPONENTS | ä»»åŠ¡éš¾åº¦å¢åŠ  | ä»»åŠ¡ç®€åŒ– | 5-10 |

### æ¨¡å‹é…ç½®

```python
MODEL_CONFIG = {
    'd_model': 128,              # æ¨¡å‹ç»´åº¦
    'nhead': 8,                  # æ³¨æ„åŠ›å¤´æ•°
    'num_layers': 4,             # ç¼–ç å™¨å±‚æ•°
    'dim_feedforward': 512,      # å‰é¦ˆç½‘ç»œç»´åº¦
    'dropout': 0.1,              # Dropoutæ¯”ä¾‹
}
```

**ä¿®æ”¹å»ºè®®**ï¼š

| å‚æ•° | å¢å¤§æ•ˆæœ | å‡å°æ•ˆæœ | æ¨èå€¼ |
|------|----------|----------|--------|
| d_model | å®¹é‡å¢å¤§ï¼Œè®­ç»ƒæ…¢ | å®¹é‡å‡å°ï¼Œè®­ç»ƒå¿« | 64-256 |
| nhead | å¤šè§’åº¦ç‰¹å¾æå– | è®¡ç®—å¿«ä½†ç‰¹å¾å•ä¸€ | 4-16 |
| num_layers | ç‰¹å¾æŠ½è±¡èƒ½åŠ›å¼º | æ¨¡å‹ç®€å•ï¼Œæ˜“è®­ç»ƒ | 2-8 |
| dim_feedforward | éçº¿æ€§èƒ½åŠ›å¼º | è®¡ç®—é«˜æ•ˆ | 256-2048 |
| dropout | é˜²æ­¢è¿‡æ‹Ÿåˆ | å¯èƒ½æ¬ æ‹Ÿåˆ | 0.1-0.5 |

**æ³¨æ„**ï¼š`nhead`å¿…é¡»æ˜¯`d_model`çš„å› å­ï¼

### è®­ç»ƒé…ç½®

```python
BATCH_SIZE = 16
EPOCHS = 100
LEARNING_RATE = 0.0001
WEIGHT_DECAY = 1e-4
EARLY_STOPPING_PATIENCE = 15
```

**ä¿®æ”¹å»ºè®®**ï¼š

| å‚æ•° | å¢å¤§æ•ˆæœ | å‡å°æ•ˆæœ | æ¨èå€¼ |
|------|----------|----------|--------|
| BATCH_SIZE | è®­ç»ƒç¨³å®šä½†å†…å­˜å ç”¨å¤§ | æ¢¯åº¦å™ªå£°å¤§ | 8-32 |
| EPOCHS | è®­ç»ƒå……åˆ†ä½†è€—æ—¶é•¿ | è®­ç»ƒå¿«ä½†å¯èƒ½æ¬ æ‹Ÿåˆ | 50-200 |
| LEARNING_RATE | æ”¶æ•›å¿«ä½†å¯èƒ½ä¸ç¨³å®š | æ”¶æ•›æ…¢ä½†æ›´ç¨³å®š | 1e-5 ~ 1e-3 |
| WEIGHT_DECAY | æ­£åˆ™åŒ–å¼ºï¼Œé˜²è¿‡æ‹Ÿåˆ | å¯èƒ½æ¬ æ‹Ÿåˆ | 1e-5 ~ 1e-3 |

### é¢„å¤„ç†é…ç½®

```python
PREPROCESSING_CONFIG = {
    'baseline_correction': True,
    'normalization': 'minmax',     # 'minmax' æˆ– 'standard'
    'baseline_method': 'poly',     # 'poly' æˆ– 'als'
    'poly_degree': 3,
}
```

**åŸºçº¿æ ¡æ­£æ–¹æ³•**ï¼š
- `poly`ï¼ˆå¤šé¡¹å¼ï¼‰ï¼šå¿«é€Ÿï¼Œé€‚åˆå¹³æ»‘åŸºçº¿
- `als`ï¼ˆè‡ªé€‚åº”æœ€å°äºŒä¹˜ï¼‰ï¼šå‡†ç¡®ï¼Œé€‚åˆå¤æ‚åŸºçº¿

**å½’ä¸€åŒ–æ–¹æ³•**ï¼š
- `minmax`ï¼šç¼©æ”¾åˆ°[0, 1]ï¼Œä¿ç•™åˆ†å¸ƒå½¢çŠ¶
- `standard`ï¼šæ ‡å‡†åŒ–åˆ°å‡å€¼0æ–¹å·®1ï¼Œé€‚åˆæœ‰å¼‚å¸¸å€¼çš„æ•°æ®

---

## å¼€å‘è€…æŒ‡å—

### å¦‚ä½•æ·»åŠ æ–°æˆåˆ†

#### æ­¥éª¤1ï¼šä¿®æ”¹é…ç½®

åœ¨`config.py`ä¸­æ·»åŠ æ–°æˆåˆ†ï¼š

```python
COMPONENT_NAMES = [
    'é˜¿å¸åŒ¹æ—',
    'å¯¹ä¹™é…°æ°¨åŸºé…š',
    'å’–å•¡å› ',
    'å¸ƒæ´›èŠ¬',
    'è˜æ™®ç”Ÿ',
    'æ–°è¯ç‰©',  # æ·»åŠ æ–°æˆåˆ†
]
NUM_COMPONENTS = 6  # æ›´æ–°æ•°é‡
```

#### æ­¥éª¤2ï¼šæ·»åŠ ç‰¹å¾å³°

åœ¨`data/data_generator.py`ä¸­çš„`_define_component_peaks`æ–¹æ³•ä¸­æ·»åŠ æ–°æˆåˆ†çš„ç‰¹å¾å³°ä½ç½®ï¼š

```python
def _define_component_peaks(self):
    """å®šä¹‰å„æˆåˆ†çš„ç‰¹å¾å³°ä½ç½®"""
    self.component_peaks = {
        0: [560, 1045, 1610],  # é˜¿å¸åŒ¹æ—
        1: [840, 1230, 1650],  # å¯¹ä¹™é…°æ°¨åŸºé…š
        2: [655, 1290, 1680],  # å’–å•¡å› 
        3: [735, 1190, 1590],  # å¸ƒæ´›èŠ¬
        4: [815, 1355, 1635],  # è˜æ™®ç”Ÿ
        5: [900, 1400, 1700],  # æ–°è¯ç‰©ï¼ˆç¤ºä¾‹å³°ä½ï¼‰
    }
```

**æ³¨æ„**ï¼šå³°ä½åº”åœ¨æ³¢æ•°èŒƒå›´400-2000 cmâ»Â¹å†…

### å¦‚ä½•è°ƒæ•´æ¨¡å‹æ¶æ„

#### ç¤ºä¾‹1ï¼šå¢åŠ Dropouté˜²æ­¢è¿‡æ‹Ÿåˆ

ä¿®æ”¹`models/transformer.py`ï¼š

```python
# åœ¨__init__æ–¹æ³•ä¸­
self.classification_head = nn.Sequential(
    nn.Linear(d_model, 64),
    nn.ReLU(),
    nn.Dropout(0.5),  # å¢å¤§dropout
    nn.Linear(64, num_components),
    nn.Sigmoid()
)
```

#### ç¤ºä¾‹2ï¼šæ·»åŠ æ‰¹å½’ä¸€åŒ–å±‚

```python
self.classification_head = nn.Sequential(
    nn.Linear(d_model, 64),
    nn.BatchNorm1d(64),  # æ·»åŠ æ‰¹å½’ä¸€åŒ–
    nn.ReLU(),
    nn.Dropout(0.3),
    nn.Linear(64, num_components),
    nn.Sigmoid()
)
```

#### ç¤ºä¾‹3ï¼šæ›´æ”¹æ¿€æ´»å‡½æ•°

```python
# å°†ReLUæ”¹ä¸ºGELU
self.classification_head = nn.Sequential(
    nn.Linear(d_model, 64),
    nn.GELU(),  # ä½¿ç”¨GELUæ¿€æ´»
    nn.Dropout(0.3),
    nn.Linear(64, num_components),
    nn.Sigmoid()
)
```

### å¦‚ä½•ä½¿ç”¨è‡ªå·±çš„æ•°æ®

#### æ–¹æ³•1ï¼šä½¿ç”¨CSVæ ¼å¼

```python
import pandas as pd
import numpy as np

# è¯»å–æ•°æ®
df = pd.read_csv('my_data.csv')

# å‡è®¾CSVæ ¼å¼ä¸º:
# - å‰1738åˆ—ä¸ºå…‰è°±æ•°æ®
# - æ¥ä¸‹æ¥5åˆ—ä¸ºæˆåˆ†æ ‡ç­¾
# - æœ€å5åˆ—ä¸ºæµ“åº¦å€¼

spectra = df.iloc[:, :1738].values
component_labels = df.iloc[:, 1738:1743].values
concentrations = df.iloc[:, 1743:].values

# é¢„å¤„ç†
from utils.preprocessing import preprocess_spectrum
spectra = preprocess_spectrum(spectra, 'poly', 'minmax', degree=3)

# åˆ›å»ºæ•°æ®é›†
from utils.dataset import SpectrumDataset
dataset = SpectrumDataset(spectra, component_labels, concentrations)
```

#### æ–¹æ³•2ï¼šä½¿ç”¨NumPyæ•°ç»„

```python
import numpy as np

# å‡è®¾æ‚¨çš„æ•°æ®å·²ç»æ˜¯numpyæ•°ç»„
# spectra: (N, 1738)
# component_labels: (N, 5)
# concentrations: (N, 5)

# ç›´æ¥åŠ è½½
spectra = np.load('my_spectra.npy')
component_labels = np.load('my_labels.npy')
concentrations = np.load('my_concentrations.npy')

# åç»­æ­¥éª¤åŒæ–¹æ³•1
```

### ä»£ç æ‰©å±•ç¤ºä¾‹

#### ç¤ºä¾‹1ï¼šæ·»åŠ æ¨¡å‹é›†æˆï¼ˆEnsembleï¼‰

```python
class EnsembleModel:
    """æ¨¡å‹é›†æˆ"""
    def __init__(self, model_paths, device):
        self.models = []
        for path in model_paths:
            model = create_model()
            model.load_state_dict(torch.load(path))
            model.eval()
            model.to(device)
            self.models.append(model)
        self.device = device
    
    def predict(self, x):
        """é›†æˆé¢„æµ‹"""
        x = x.to(self.device)
        comp_preds = []
        conc_preds = []
        
        with torch.no_grad():
            for model in self.models:
                comp, conc = model(x)
                comp_preds.append(comp)
                conc_preds.append(conc)
        
        # å¹³å‡é›†æˆ
        comp_avg = torch.stack(comp_preds).mean(dim=0)
        conc_avg = torch.stack(conc_preds).mean(dim=0)
        
        return comp_avg, conc_avg

# ä½¿ç”¨
ensemble = EnsembleModel([
    'saved_models/model1.pth',
    'saved_models/model2.pth',
    'saved_models/model3.pth'
], config.DEVICE)

comp_pred, conc_pred = ensemble.predict(test_data)
```

#### ç¤ºä¾‹2ï¼šæ·»åŠ æ³¨æ„åŠ›å¯è§†åŒ–

```python
def visualize_attention(model, spectrum, layer_idx=0):
    """å¯è§†åŒ–æŒ‡å®šå±‚çš„æ³¨æ„åŠ›æƒé‡"""
    import matplotlib.pyplot as plt
    
    model.eval()
    spectrum_tensor = torch.FloatTensor(spectrum).unsqueeze(0).to(config.DEVICE)
    
    # æ³¨å†Œhookè·å–æ³¨æ„åŠ›æƒé‡
    attention_weights = []
    
    def hook_fn(module, input, output):
        # outputæ˜¯(attn_output, attn_weights)
        attention_weights.append(output[1].detach().cpu())
    
    # æ³¨å†Œåˆ°æŒ‡å®šå±‚
    hook = model.transformer_encoder.layers[layer_idx].self_attn.register_forward_hook(hook_fn)
    
    # å‰å‘ä¼ æ’­
    with torch.no_grad():
        _ = model(spectrum_tensor)
    
    hook.remove()
    
    # ç»˜åˆ¶æ³¨æ„åŠ›å›¾
    attn = attention_weights[0][0]  # (nhead, seq_len, seq_len)
    attn_avg = attn.mean(dim=0)  # å¹³å‡æ‰€æœ‰å¤´
    
    plt.figure(figsize=(10, 8))
    plt.imshow(attn_avg.numpy(), cmap='viridis', aspect='auto')
    plt.colorbar(label='æ³¨æ„åŠ›æƒé‡')
    plt.xlabel('é”®ä½ç½®')
    plt.ylabel('æŸ¥è¯¢ä½ç½®')
    plt.title(f'ç¬¬{layer_idx}å±‚æ³¨æ„åŠ›å›¾')
    plt.savefig(f'attention_layer_{layer_idx}.png', dpi=150, transparent=True)
    plt.close()

# ä½¿ç”¨
visualize_attention(model, processed_spectra[0], layer_idx=0)
```

---

## å¸¸è§é—®é¢˜

### è®­ç»ƒç›¸å…³

**Q1: è®­ç»ƒæŸå¤±ä¸ä¸‹é™**

A: å¯èƒ½çš„åŸå› å’Œè§£å†³æ–¹æ¡ˆï¼š
1. å­¦ä¹ ç‡è¿‡å¤§æˆ–è¿‡å°
   ```python
   LEARNING_RATE = 0.0001  # å°è¯•è°ƒæ•´
   ```
2. æ¨¡å‹è¿‡äºå¤æ‚æˆ–ç®€å•
   ```python
   MODEL_CONFIG['num_layers'] = 2  # å‡å°‘å±‚æ•°
   ```
3. æ•°æ®æœªå½’ä¸€åŒ–
   ```python
   # ç¡®ä¿ä½¿ç”¨äº†preprocess_spectrum
   ```

**Q2: è®­ç»ƒæ—¶æ˜¾å­˜/å†…å­˜ä¸è¶³**

A: è§£å†³æ–¹æ¡ˆï¼š
1. å‡å°æ‰¹æ¬¡å¤§å°
   ```python
   BATCH_SIZE = 8  # ä»16å‡åˆ°8
   ```
2. å‡å°æ¨¡å‹è§„æ¨¡
   ```python
   MODEL_CONFIG['d_model'] = 64  # ä»128å‡åˆ°64
   ```
3. ä½¿ç”¨CPUæ¨¡å¼
   ```python
   DEVICE_MODE = 'cpu'
   ```

**Q3: éªŒè¯é›†æ€§èƒ½ä¸æå‡**

A: å¯èƒ½å‡ºç°è¿‡æ‹Ÿåˆï¼Œå°è¯•ï¼š
1. å¢å¤§Dropout
   ```python
   MODEL_CONFIG['dropout'] = 0.3  # ä»0.1å¢å¤§åˆ°0.3
   ```
2. å¢åŠ æ•°æ®é‡
   ```python
   NUM_MIXTURES = 160  # å¢åŠ åˆ°160
   ```
3. ä½¿ç”¨æ­£åˆ™åŒ–
   ```python
   WEIGHT_DECAY = 1e-3  # å¢å¤§æƒé‡è¡°å‡
   ```

### é¢„æµ‹ç›¸å…³

**Q4: æˆåˆ†è¯†åˆ«å‡†ç¡®ä½†æµ“åº¦é¢„æµ‹ä¸å‡†**

A: æµ“åº¦é¢„æµ‹æ¯”åˆ†ç±»æ›´éš¾ï¼Œå°è¯•ï¼š
1. è°ƒæ•´ä»»åŠ¡æƒé‡
   ```python
   TASK_WEIGHTS = {
       'classification': 0.5,
       'regression': 1.5,  # å¢å¤§å›å½’æƒé‡
   }
   ```
2. å•ç‹¬è°ƒæ•´å›å½’å¤´çš„Dropout
3. å¢åŠ è®­ç»ƒè½®æ•°

**Q5: é¢„æµ‹ç»“æœä¸­æµ“åº¦å’Œä¸ä¸º1**

A: æœ¬é¡¹ç›®çš„å›å½’å¤´ä½¿ç”¨äº†Softmaxç¡®ä¿æµ“åº¦å½’ä¸€åŒ–ï¼Œä½†éœ€è¦æ³¨æ„ï¼š
- åªå¯¹é¢„æµ‹ä¸ºå­˜åœ¨çš„æˆåˆ†è®¡ç®—æµ“åº¦
- ç¡®è®¤æ¨¡å‹æ˜¯æœ€æ–°ç‰ˆæœ¬ï¼ˆåŒ…å«Softmaxå±‚ï¼‰

### ç¯å¢ƒç›¸å…³

**Q6: CUDAç›¸å…³é”™è¯¯**

A: GPUå…¼å®¹æ€§é—®é¢˜ï¼š
1. åˆ‡æ¢åˆ°CPUæ¨¡å¼
   ```python
   DEVICE_MODE = 'cpu'
   ```
2. é‡æ–°å®‰è£…PyTorch
   ```bash
   # è®¿é—® https://pytorch.org/ è·å–æ­£ç¡®ç‰ˆæœ¬
   ```

**Q7: ä¸­æ–‡æ˜¾ç¤ºä¹±ç **

A: å·²åœ¨ä»£ç ä¸­é…ç½®ï¼Œå¦‚ä»æœ‰é—®é¢˜ï¼š
```python
import matplotlib.pyplot as plt
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei']
plt.rcParams['axes.unicode_minus'] = False
```

**Q8: å¯¼å…¥æ¨¡å—å¤±è´¥**

A: ç¡®ä¿ï¼š
1. å·²æ¿€æ´»æ­£ç¡®çš„ç¯å¢ƒ
   ```bash
   conda activate use
   ```
2. åœ¨é¡¹ç›®æ ¹ç›®å½•è¿è¡Œ
   ```bash
   cd "D:\...\Transformeræ¨¡å‹"
   ```
3. å·²å®‰è£…æ‰€æœ‰ä¾èµ–
   ```bash
   pip install -r requirements.txt
   ```

---

## æ€§èƒ½ä¼˜åŒ–å»ºè®®

### è®­ç»ƒä¼˜åŒ–

#### 1. ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼ˆGPUï¼‰

å¦‚æœæœ‰GPUï¼Œå¯ä»¥ä½¿ç”¨æ··åˆç²¾åº¦åŠ é€Ÿè®­ç»ƒï¼š

```python
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

for epoch in range(EPOCHS):
    for batch in train_loader:
        optimizer.zero_grad()
        
        # æ··åˆç²¾åº¦å‰å‘ä¼ æ’­
        with autocast():
            comp_pred, conc_pred = model(batch_spectra)
            loss, _, _ = calculate_loss(comp_pred, batch_labels, conc_pred, batch_conc)
        
        # æ··åˆç²¾åº¦åå‘ä¼ æ’­
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()
```

**æ•ˆæœ**ï¼šè®­ç»ƒé€Ÿåº¦æå‡30-50%ï¼Œæ˜¾å­˜å ç”¨å‡å°‘

#### 2. ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯

å†…å­˜ä¸è¶³æ—¶ï¼Œå¯ä»¥é€šè¿‡æ¢¯åº¦ç´¯ç§¯æ¨¡æ‹Ÿæ›´å¤§çš„æ‰¹æ¬¡ï¼š

```python
accumulation_steps = 4  # ç´¯ç§¯4ä¸ªbatch

for i, batch in enumerate(train_loader):
    # å‰å‘ä¼ æ’­
    comp_pred, conc_pred = model(batch_spectra)
    loss, _, _ = calculate_loss(comp_pred, batch_labels, conc_pred, batch_conc)
    
    # å½’ä¸€åŒ–æŸå¤±
    loss = loss / accumulation_steps
    loss.backward()
    
    # æ¯accumulation_stepsæ­¥æ›´æ–°ä¸€æ¬¡
    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()
```

**æ•ˆæœ**ï¼šç›¸å½“äºä½¿ç”¨BATCH_SIZE Ã— accumulation_stepsçš„æ‰¹æ¬¡å¤§å°

#### 3. æ•°æ®åŠ è½½ä¼˜åŒ–

```python
train_loader = DataLoader(
    train_dataset,
    batch_size=BATCH_SIZE,
    shuffle=True,
    num_workers=4,  # å¤šè¿›ç¨‹åŠ è½½
    pin_memory=True,  # å¦‚æœä½¿ç”¨GPU
    prefetch_factor=2  # é¢„åŠ è½½
)
```

**æ•ˆæœ**ï¼šå‡å°‘æ•°æ®åŠ è½½ç­‰å¾…æ—¶é—´

### æ¨ç†ä¼˜åŒ–

#### 1. æ‰¹é‡æ¨ç†

```python
# ä¸å¥½çš„åšæ³•ï¼šé€ä¸ªæ ·æœ¬æ¨ç†
for spectrum in test_spectra:
    pred = model(spectrum.unsqueeze(0))

# å¥½çš„åšæ³•ï¼šæ‰¹é‡æ¨ç†
batch_size = 32
for i in range(0, len(test_spectra), batch_size):
    batch = test_spectra[i:i+batch_size]
    preds = model(batch)
```

**æ•ˆæœ**ï¼šæ¨ç†é€Ÿåº¦æå‡10-20å€

#### 2. æ¨¡å‹é‡åŒ–ï¼ˆCPUæ¨ç†ï¼‰

```python
import torch.quantization

# åŠ¨æ€é‡åŒ–
model_quantized = torch.quantization.quantize_dynamic(
    model, {nn.Linear}, dtype=torch.qint8
)

# ä½¿ç”¨é‡åŒ–æ¨¡å‹
comp_pred, conc_pred = model_quantized(test_data)
```

**æ•ˆæœ**ï¼šæ¨ç†é€Ÿåº¦æå‡2-4å€ï¼Œæ¨¡å‹å¤§å°å‡å°‘75%

#### 3. TorchScriptå¯¼å‡º

```python
# è½¬æ¢ä¸ºTorchScript
model.eval()
example_input = torch.randn(1, 1738).to(config.DEVICE)
traced_model = torch.jit.trace(model, example_input)

# ä¿å­˜
traced_model.save('model_traced.pt')

# åŠ è½½ä½¿ç”¨
loaded_model = torch.jit.load('model_traced.pt')
pred = loaded_model(test_data)
```

**æ•ˆæœ**ï¼šæ¨ç†é€Ÿåº¦æå‡10-30%

---

## é™„å½•

### A. æœ¯è¯­è¡¨

| æœ¯è¯­ | è¯´æ˜ |
|------|------|
| **Transformer** | åŸºäºè‡ªæ³¨æ„åŠ›æœºåˆ¶çš„æ·±åº¦å­¦ä¹ æ¶æ„ |
| **Self-Attention** | è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œè®¡ç®—åºåˆ—å†…éƒ¨å…ƒç´ ä¹‹é—´çš„å…³ç³» |
| **Multi-Head Attention** | å¤šå¤´æ³¨æ„åŠ›ï¼Œä»å¤šä¸ªè§’åº¦æå–ç‰¹å¾ |
| **Positional Encoding** | ä½ç½®ç¼–ç ï¼Œä¸ºæ¨¡å‹æä¾›åºåˆ—ä½ç½®ä¿¡æ¯ |
| **Encoder-only** | ä»…ä½¿ç”¨ç¼–ç å™¨çš„Transformeræ¶æ„ |
| **å¤šæ ‡ç­¾åˆ†ç±»** | æ¯ä¸ªæ ·æœ¬å¯ä»¥æœ‰å¤šä¸ªæ ‡ç­¾ |
| **å¤šç›®æ ‡å›å½’** | åŒæ—¶é¢„æµ‹å¤šä¸ªè¿ç»­å€¼ |
| **æ—©åœ** | å½“éªŒè¯é›†æ€§èƒ½ä¸å†æå‡æ—¶æå‰åœæ­¢è®­ç»ƒ |
| **å­¦ä¹ ç‡è°ƒåº¦** | æ ¹æ®è®­ç»ƒè¿›åº¦åŠ¨æ€è°ƒæ•´å­¦ä¹ ç‡ |

### B. å‚è€ƒèµ„æ–™

**æ·±åº¦å­¦ä¹ **ï¼š
- PyTorchå®˜æ–¹æ–‡æ¡£ï¼šhttps://pytorch.org/docs/stable/index.html
- TransformeråŸè®ºæ–‡ï¼š"Attention is All You Need"

**æ‹‰æ›¼å…‰è°±**ï¼š
- æ‹‰æ›¼å…‰è°±åŸºç¡€çŸ¥è¯†
- åŒ–å­¦è®¡é‡å­¦æ–¹æ³•

**å¤šä»»åŠ¡å­¦ä¹ **ï¼š
- "An Overview of Multi-Task Learning in Deep Neural Networks"

### C. é¡¹ç›®ä¿¡æ¯

- **ç‰ˆæœ¬**ï¼šv1.1
- **æœ€åæ›´æ–°**ï¼š2025-10-27
- **Pythonç‰ˆæœ¬**ï¼š3.8 - 3.11
- **ä¸»è¦ä¾èµ–**ï¼šPyTorch 1.10+, NumPy, Matplotlib, scikit-learn

---

## ä¸‹ä¸€æ­¥å»ºè®®

å®Œæˆæœ¬æŒ‡å—çš„å­¦ä¹ åï¼Œæ‚¨å¯ä»¥ï¼š

1. **ä¿®æ”¹é…ç½®**ï¼šå°è¯•ä¸åŒçš„è¶…å‚æ•°ç»„åˆ
2. **ä½¿ç”¨è‡ªå·±çš„æ•°æ®**ï¼šå°†æ–¹æ³•åº”ç”¨åˆ°å®é™…é—®é¢˜
3. **æ”¹è¿›æ¨¡å‹**ï¼šå°è¯•ä¸åŒçš„æ¶æ„å˜ä½“
4. **æ‰©å±•åŠŸèƒ½**ï¼šæ·»åŠ å¯è§†åŒ–ã€æ¨¡å‹é›†æˆç­‰åŠŸèƒ½
5. **æ€§èƒ½ä¼˜åŒ–**ï¼šåº”ç”¨æœ¬æŒ‡å—ä¸­çš„ä¼˜åŒ–æŠ€å·§

---

**ç¥æ‚¨ä½¿ç”¨æ„‰å¿«ï¼** ğŸ‰

å¦‚æœ‰é—®é¢˜ï¼Œè¯·å‚è€ƒï¼š
- [ç¯å¢ƒé…ç½®.md](ç¯å¢ƒé…ç½®.md) - ç¯å¢ƒé…ç½®é—®é¢˜
- [README.md](README.md) - é¡¹ç›®æ¦‚è§ˆ
- ä»£ç ä¸­çš„è¯¦ç»†æ³¨é‡Š
